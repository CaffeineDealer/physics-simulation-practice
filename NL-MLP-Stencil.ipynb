{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f83eaba",
   "metadata": {},
   "source": [
    "Nonlinear MLP Stencil\n",
    "\n",
    "Goal: Use MLP to train nonlinear stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.func import jacrev, vmap, functional_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339181c",
   "metadata": {},
   "source": [
    "Define MLP to output stencil perturbation and build nonlinear residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ebd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StencilMLP(nn.Module):\n",
    "    \"\"\"Map local stencil to perturbation coeff using MLP\n",
    "    Input: local solution\n",
    "    Output: perturbation weights\n",
    "    \"\"\"\n",
    "    def __init__(self, stencil_width=3, hidden=16):\n",
    "        super().__init__() # initialize parent\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(stencil_width, hidden), # layer1: input\n",
    "            nn.Tanh(), # layer2: activation func\n",
    "            nn.Linear(hidden, hidden), # layer3: hidden to hidden\n",
    "            nn.Tanh(), # layer4: activation func\n",
    "            nn.Linear(hidden, stencil_width), # layer 5: map hidden to output\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight.zero_() # set all weights in the final layer to zero\n",
    "            self.net[-1].bias.zero_() # set all biases to zero\n",
    "\n",
    "    def forward(self, u_local):\n",
    "        return self.net(u_local)\n",
    "    \n",
    "def extract_stencil_window(u, stencil_radius=1):\n",
    "    # Extract all local stencil windows from u with periodic BCs\n",
    "    N = u.shape[0]\n",
    "    width = 2 * stencil_radius + 1\n",
    "    u_padded = torch.cat([u[-stencil_radius:], u, u[:stencil_radius]])\n",
    "    windows = u_padded.unfold(0, width, 1)\n",
    "    return windows\n",
    "\n",
    "def apply_nonlinear_stencil(u, mlp_params, mlp_buffers, mlp_forward, base_coeffs):\n",
    "    # Apply nonlinear stencil to the full solution vector u\n",
    "    windows = extract_stencil_window(u)\n",
    "    delta_a = mlp_forward(mlp_params, mlp_buffers, windows)\n",
    "    a_eff = base_coeffs.unsqueeze(0) + delta_a\n",
    "    Lu = (a_eff * windows).sum(dim=1)\n",
    "    return Lu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b67f1e",
   "metadata": {},
   "source": [
    "Implicit Euler Residual and Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residual u_new - u_old - dt * Lu for the full system\n",
    "def implicit_euler_residual(u_new, u_old, dt, mlp_params, mlp_buffers,\n",
    "                            mlp_forward, base_coeffs):\n",
    "    Lu = apply_nonlinear_stencil(u_new, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "    return u_new - u_old - dt * Lu\n",
    "\n",
    "# Compute the residual for a single spatial point i using its local stencil (3 points)\n",
    "def local_residual_i(u_local_3, i, u_full ,u_old_i, dt, mlp_buffers, mlp_forward, base_coeefs):\n",
    "    delta_a = mlp_forward(mlp_params, mlp_buffers, u_local_3.unsqueeze(0)).squeeze(0) \n",
    "    a_eff = base_coeffs + delta_a  \n",
    "    Lu_i = (a_eff * u_local_3).sum()\n",
    "    return u_local_3[1] - u_old_i - dt * Lu_i \n",
    "\n",
    "# Build Jacobian Matrix\n",
    "def assemble_jacobian_banded(u_new, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs):\n",
    "    N = u_new.shape[0]\n",
    "    windows = extract_stencil_window(u_new)\n",
    "\n",
    "    def local_res_fn(u_local_3, u_old_i):\n",
    "        delta_a = mlp_forward(mlp_params, mlp_buffers, u_local_3.unsqueeze(0)).squeeze(0)\n",
    "        a_eff = base_coeffs + delta_a\n",
    "        Li_i = (a_eff * u_local_3).sum()\n",
    "        return u_local_3[1] - u_old_i - dt * Lu_i\n",
    "    \n",
    "    local_jac_fn = jacrev(local_res_fn, argums=0)\n",
    "    all_loacl_jacs = vmap(local_jac_fn)(windowns, u_old)\n",
    "\n",
    "    J = torch.zeros(N, N, dtype=u_new.dtype, device=u_new.device)\n",
    "    idx = torch.arrange(N, device=u_new.device)\n",
    "\n",
    "    for k in range(3):\n",
    "        col = (idx + (k - 1)) % N\n",
    "        J[idx, col] = all_loacl_jacs[:, k]\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600a6c3",
   "metadata": {},
   "source": [
    "Newton Solver with Armijo backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915c23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_solve(u_init, u_old, dt, mlp_params, mlp_buffers, mlp_forward,\n",
    "                 base_coeffs, tol=1e-10, max_iter=20, verbose=False):\n",
    "    \n",
    "    u = u_init.detach().clone()\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        with torch.no_grad():\n",
    "            F_val = implicit_euler_residual(u, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "\n",
    "            res_norm = F_val.norm().item()\n",
    "            if verbose:\n",
    "                print(f\"Newton iter {k}: ||F|| = {res_norm:.3e}\")\n",
    "                if res_norm < tol:\n",
    "                    break\n",
    "        with torch.enable_grad():\n",
    "            J = assemble_jacobian_banded(u, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "        with torch.no_grad():\n",
    "            delta_u = torch.linalg.solve(J.detach(), -F_val)\n",
    "\n",
    "            alpha = 1.0\n",
    "            c1 = 1e-4\n",
    "            tau = 0.5\n",
    "            phi_current = 0.5 * F_val.dot(F_val)\n",
    "            directional_deriv = -F_val.dot(F_val)\n",
    "\n",
    "            for _ in range(20):\n",
    "                u_trial = u + alpha * delta_u\n",
    "                F_trial = implicit_euler_residual(u_trial, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "                phi_trial = 0.5 * F_trial.dot(F_trial)\n",
    "                if phi_trial <= phi_current + c1 * alpha * directional_deriv:\n",
    "                    break\n",
    "                alpha *= tau\n",
    "\n",
    "            u = u + alpha * delta_u\n",
    "\n",
    "    u_detached = u.detach()\n",
    "    F_val = implicit_euler_residual(u_detached, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "    J = assemble_jacobian_banded(u_detached, u_old, dt, mlp_params, mlp_buffers, mlp_forward, base_coeffs)\n",
    "    delta_u = torch.linalg.solve(J, -F_val)\n",
    "    u_out = u_detached + delta_u\n",
    "\n",
    "    return u_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5321e34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physim (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
